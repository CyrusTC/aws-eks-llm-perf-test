# AWS EKS LLM Performance Test

Infrastructure for deploying and testing LLM performance on AWS EKS with GPU nodes.

## Architecture

- **VPC**: 3 AZs with public/private subnets
- **EKS Cluster**: Kubernetes 1.33 with Karpenter for GPU node provisioning
- **GPU Nodes**: G5.8xlarge instances with NVIDIA A10G GPUs
- **LLM Model**: GPT-OSS-20b served via vLLM

## FlashAttention 3 Compatibility Issue

### Problem
GPT-OSS-20b model requires FlashAttention 3 support, causing this error on A10G GPUs:
```
AssertionError: Sinks are only supported in FlashAttention 3
```

### Solution
Use Triton attention backend instead of FlashAttention by setting the environment variable:
```bash
VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1
```

This is implemented in the Terraform deployment configuration:
```hcl
command = [
  "bash", "-c",
  <<-EOT
  uv venv --python 3.12 --seed && \
  source .venv/bin/activate && \
  uv pip install --pre vllm \
    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \
    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \
    --index-strategy unsafe-best-match && \
  VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1 vllm serve openai/gpt-oss-20b --max_model 100000
  EOT
]
```

## Deployment

```bash
terraform init
terraform apply
```

## Performance Testing

The infrastructure includes a persistent Triton container for manual performance testing:

### Running Performance Tests

1. **Access the Triton container**:
   ```bash
   kubectl exec -it deployment/triton-perf -n llm -- bash
   ```

2. **Run basic inference test**:
   ```bash
   curl -X POST http://llm-gpu.llm.svc.cluster.local:8000/v1/completions \
     -H "Content-Type: application/json" \
     -d '{
       "model": "openai/gpt-oss-20b",
       "prompt": "The capital of France is",
       "max_tokens": 10,
       "temperature": 0.7
     }'
   ```

3. **Run comprehensive performance tests by concurrency**:

   **Concurrency 20 (Baseline)**:
   ```bash
   genai-perf profile -m openai/gpt-oss-20b \
     --url http://llm-gpu.llm.svc.cluster.local:8000 \
     --service-kind openai \
     --endpoint-type completions \
     --num-prompts 100 \
     --synthetic-input-tokens-mean 200 \
     --synthetic-input-tokens-stddev 20 \
     --output-tokens-mean 100 \
     --output-tokens-stddev 10 \
     --concurrency 20 \
     --tokenizer hf-internal-testing/llama-tokenizer \
     --generate-plots
   ```

   **Concurrency 40 (Medium Load)**:
   ```bash
   genai-perf profile -m openai/gpt-oss-20b \
     --url http://llm-gpu.llm.svc.cluster.local:8000 \
     --service-kind openai \
     --endpoint-type completions \
     --num-prompts 200 \
     --synthetic-input-tokens-mean 200 \
     --synthetic-input-tokens-stddev 20 \
     --output-tokens-mean 100 \
     --output-tokens-stddev 10 \
     --concurrency 40 \
     --tokenizer hf-internal-testing/llama-tokenizer \
     --generate-plots
   ```

   **Concurrency 80 (High Load)**:
   ```bash
   genai-perf profile -m openai/gpt-oss-20b \
     --url http://llm-gpu.llm.svc.cluster.local:8000 \
     --service-kind openai \
     --endpoint-type completions \
     --num-prompts 400 \
     --synthetic-input-tokens-mean 200 \
     --synthetic-input-tokens-stddev 20 \
     --output-tokens-mean 100 \
     --output-tokens-stddev 10 \
     --concurrency 80 \
     --tokenizer hf-internal-testing/llama-tokenizer \
     --generate-plots
   ```

   **Concurrency 120 (Very High Load)**:
   ```bash
   genai-perf profile -m openai/gpt-oss-20b \
     --url http://llm-gpu.llm.svc.cluster.local:8000 \
     --service-kind openai \
     --endpoint-type completions \
     --num-prompts 600 \
     --synthetic-input-tokens-mean 200 \
     --synthetic-input-tokens-stddev 20 \
     --output-tokens-mean 100 \
     --output-tokens-stddev 10 \
     --concurrency 120 \
     --tokenizer hf-internal-testing/llama-tokenizer \
     --generate-plots
   ```

   **Concurrency 200 (Maximum Load)**:
   ```bash
   genai-perf profile -m openai/gpt-oss-20b \
     --url http://llm-gpu.llm.svc.cluster.local:8000 \
     --service-kind openai \
     --endpoint-type completions \
     --num-prompts 1000 \
     --synthetic-input-tokens-mean 200 \
     --synthetic-input-tokens-stddev 20 \
     --output-tokens-mean 100 \
     --output-tokens-stddev 10 \
     --concurrency 200 \
     --tokenizer hf-internal-testing/llama-tokenizer \
     --generate-plots
   ```

### Test Parameters by Concurrency Level
| Concurrency | Prompts | Input Tokens | Output Tokens | Expected Duration |
|-------------|---------|--------------|---------------|-------------------|
| 20          | 100     | 200 Â± 20     | 100 Â± 10      | ~2-3 minutes      |
| 40          | 200     | 200 Â± 20     | 100 Â± 10      | ~3-4 minutes      |
| 80          | 400     | 200 Â± 20     | 100 Â± 10      | ~5-6 minutes      |
| 120         | 600     | 200 Â± 20     | 100 Â± 10      | ~7-8 minutes      |
| 200         | 1000    | 200 Â± 20     | 100 Â± 10      | ~10-12 minutes    |

### Key Metrics to Monitor
- **Throughput**: Requests per second
- **Latency**: P50, P95, P99 response times
- **Token throughput**: Input/output tokens per second
- **Error rate**: Failed requests percentage
- **GPU utilization**: Memory and compute usage

The Triton container runs persistently, so you can run multiple tests and compare results across different concurrency levels.

### Automated Performance Testing Script

For comprehensive testing across multiple concurrency levels, use this automated script:

```bash
#!/bin/bash

# Configuration
CONCURRENCY_LEVELS=(10 20 40 80 120 200)
INSTANCE_SIZE="G5.8xlarge"
INPUT_TOKEN_SIZE=200
NUM_PROMPTS=100
MODEL_NAME="openai/gpt-oss-20b"
SERVICE_URL="http://llm-gpu.llm.svc.cluster.local:8000"

echo "=== Automated Performance Testing ==="
echo "Instance Size: $INSTANCE_SIZE"
echo "Model: $MODEL_NAME"
echo "Input Token Size: $INPUT_TOKEN_SIZE"
echo "Number of Prompts: $NUM_PROMPTS"
echo "Concurrency Levels: ${CONCURRENCY_LEVELS[@]}"
echo "Started at: $(date)"
echo "======================================="

# Loop through concurrency levels
for concurrency in "${CONCURRENCY_LEVELS[@]}"; do
    echo ""
    echo "ðŸš€ Starting test: Concurrency $concurrency on $INSTANCE_SIZE"
    echo "â° Test started at: $(date)"
    
    # Calculate prompts based on concurrency for better load distribution
    test_prompts=$((NUM_PROMPTS * concurrency / 10))
    
    echo "ðŸ“Š Test parameters:"
    echo "   - Concurrency: $concurrency"
    echo "   - Prompts: $test_prompts"
    echo "   - Input tokens: $INPUT_TOKEN_SIZE Â± 20"
    echo "   - Output tokens: 100 Â± 10"
    
    # Run genai-perf test
    genai-perf profile -m $MODEL_NAME \
        --url $SERVICE_URL \
        --service-kind openai \
        --endpoint-type completions \
        --num-prompts $test_prompts \
        --synthetic-input-tokens-mean $INPUT_TOKEN_SIZE \
        --synthetic-input-tokens-stddev 20 \
        --output-tokens-mean 100 \
        --output-tokens-stddev 10 \
        --concurrency $concurrency \
        --tokenizer hf-internal-testing/llama-tokenizer \
        --generate-plots
    
    echo "âœ… Completed test: Concurrency $concurrency"
    echo "â° Test completed at: $(date)"
    echo "---"
    
    # Optional: Add delay between tests to allow system to stabilize
    sleep 30
done

echo ""
echo "ðŸŽ‰ All performance tests completed!"
echo "â° Full test suite finished at: $(date)"
echo "ðŸ“ Results saved in artifacts/ directory"
```

**Usage:**
1. Access the Triton container: `kubectl exec -it deployment/triton-perf -n llm -- bash`
2. Create the script: `nano perf_test_suite.sh`
3. Copy the script content above
4. Make executable: `chmod +x perf_test_suite.sh`
5. Run: `./perf_test_suite.sh`

**Script Features:**
- **Configurable parameters** at the top for easy modification
- **Automatic prompt scaling** based on concurrency level
- **Detailed logging** with timestamps and test parameters
- **Progress tracking** through the test suite
- **Stabilization delays** between tests
- **Results organization** in artifacts directory

## Performance Results

### Sample Test Results (G5.8xlarge - Concurrency 10)

```
                                   NVIDIA GenAI-Perf | LLM Metrics
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                         Statistic â”ƒ      avg â”ƒ    min â”ƒ       max â”ƒ      p99 â”ƒ      p90 â”ƒ      p75 â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©
â”‚              Request latency (ms) â”‚ 5,043.38 â”‚ 465.89 â”‚ 10,071.94 â”‚ 9,903.36 â”‚ 8,738.60 â”‚ 4,975.83 â”‚
â”‚            Output sequence length â”‚   124.18 â”‚  12.00 â”‚    285.00 â”‚   193.88 â”‚   143.60 â”‚   134.00 â”‚
â”‚             Input sequence length â”‚   203.31 â”‚ 155.00 â”‚    255.00 â”‚   247.63 â”‚   228.60 â”‚   215.50 â”‚
â”‚ Output token throughput (per sec) â”‚   236.62 â”‚    N/A â”‚       N/A â”‚      N/A â”‚      N/A â”‚      N/A â”‚
â”‚      Request throughput (per sec) â”‚     1.91 â”‚    N/A â”‚       N/A â”‚      N/A â”‚      N/A â”‚      N/A â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Sample Test Results (G5.8xlarge - Concurrency 20)

```
                                    NVIDIA GenAI-Perf | LLM Metrics
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                         Statistic â”ƒ      avg â”ƒ      min â”ƒ      max â”ƒ      p99 â”ƒ      p90 â”ƒ      p75 â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©
â”‚              Request latency (ms) â”‚ 4,305.56 â”‚ 2,064.23 â”‚ 5,391.71 â”‚ 5,345.23 â”‚ 5,046.14 â”‚ 4,645.27 â”‚
â”‚            Output sequence length â”‚   125.45 â”‚    62.00 â”‚   255.00 â”‚   188.08 â”‚   148.20 â”‚   132.00 â”‚
â”‚             Input sequence length â”‚   202.12 â”‚   151.00 â”‚   255.00 â”‚   248.62 â”‚   231.00 â”‚   212.50 â”‚
â”‚ Output token throughput (per sec) â”‚   554.70 â”‚      N/A â”‚      N/A â”‚      N/A â”‚      N/A â”‚      N/A â”‚
â”‚      Request throughput (per sec) â”‚     4.42 â”‚      N/A â”‚      N/A â”‚      N/A â”‚      N/A â”‚      N/A â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Sample Test Results (G5.8xlarge - Concurrency 40)

```
                                   NVIDIA GenAI-Perf | LLM Metrics
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                         Statistic â”ƒ      avg â”ƒ    min â”ƒ       max â”ƒ      p99 â”ƒ      p90 â”ƒ      p75 â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©
â”‚              Request latency (ms) â”‚ 6,090.43 â”‚ 384.13 â”‚ 10,079.55 â”‚ 9,274.37 â”‚ 8,173.34 â”‚ 7,191.06 â”‚
â”‚            Output sequence length â”‚   125.30 â”‚   2.00 â”‚    396.00 â”‚   190.97 â”‚   144.30 â”‚   133.00 â”‚
â”‚             Input sequence length â”‚   202.69 â”‚ 151.00 â”‚    255.00 â”‚   244.81 â”‚   231.00 â”‚   217.00 â”‚
â”‚ Output token throughput (per sec) â”‚   766.65 â”‚    N/A â”‚       N/A â”‚      N/A â”‚      N/A â”‚      N/A â”‚
â”‚      Request throughput (per sec) â”‚     6.15 â”‚    N/A â”‚       N/A â”‚      N/A â”‚      N/A â”‚      N/A â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Sample Test Results (G5.8xlarge - Concurrency 80)

```
                                      NVIDIA GenAI-Perf | LLM Metrics
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                         Statistic â”ƒ      avg â”ƒ      min â”ƒ       max â”ƒ       p99 â”ƒ       p90 â”ƒ       p75 â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚              Request latency (ms) â”‚ 9,580.24 â”‚ 4,041.25 â”‚ 21,567.38 â”‚ 20,922.32 â”‚ 17,528.76 â”‚ 12,751.37 â”‚
â”‚            Output sequence length â”‚   126.02 â”‚    75.00 â”‚    596.00 â”‚    221.45 â”‚    144.50 â”‚    132.25 â”‚
â”‚             Input sequence length â”‚   201.64 â”‚   151.00 â”‚    255.00 â”‚    243.65 â”‚    228.50 â”‚    215.25 â”‚
â”‚ Output token throughput (per sec) â”‚   828.76 â”‚      N/A â”‚       N/A â”‚       N/A â”‚       N/A â”‚       N/A â”‚
â”‚      Request throughput (per sec) â”‚     6.58 â”‚      N/A â”‚       N/A â”‚       N/A â”‚       N/A â”‚       N/A â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Sample Test Results (G5.8xlarge - Concurrency 120)

```
                                      NVIDIA GenAI-Perf | LLM Metrics
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                         Statistic â”ƒ      avg â”ƒ      min â”ƒ       max â”ƒ       p99 â”ƒ       p90 â”ƒ       p75 â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚              Request latency (ms) â”‚ 9,945.11 â”‚ 5,875.81 â”‚ 22,212.10 â”‚ 19,997.09 â”‚ 15,565.79 â”‚ 11,282.18 â”‚
â”‚            Output sequence length â”‚   129.28 â”‚    79.00 â”‚    638.00 â”‚    319.57 â”‚    149.10 â”‚    131.00 â”‚
â”‚             Input sequence length â”‚   200.88 â”‚   139.00 â”‚    255.00 â”‚    243.21 â”‚    229.00 â”‚    215.00 â”‚
â”‚ Output token throughput (per sec) â”‚ 1,007.91 â”‚      N/A â”‚       N/A â”‚       N/A â”‚       N/A â”‚       N/A â”‚
â”‚      Request throughput (per sec) â”‚     7.80 â”‚      N/A â”‚       N/A â”‚       N/A â”‚       N/A â”‚       N/A â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Sample Test Results (G5.8xlarge - Concurrency 200)

```
                                      NVIDIA GenAI-Perf | LLM Metrics
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ                         Statistic â”ƒ       avg â”ƒ      min â”ƒ       max â”ƒ       p99 â”ƒ       p90 â”ƒ       p75 â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚              Request latency (ms) â”‚ 17,846.00 â”‚ 5,862.05 â”‚ 30,787.32 â”‚ 30,231.19 â”‚ 22,843.58 â”‚ 18,836.35 â”‚
â”‚            Output sequence length â”‚    125.66 â”‚    26.00 â”‚    436.00 â”‚    244.40 â”‚    144.00 â”‚    133.00 â”‚
â”‚             Input sequence length â”‚    200.98 â”‚   151.00 â”‚    255.00 â”‚    246.40 â”‚    229.00 â”‚    215.00 â”‚
â”‚ Output token throughput (per sec) â”‚    771.25 â”‚      N/A â”‚       N/A â”‚       N/A â”‚       N/A â”‚       N/A â”‚
â”‚      Request throughput (per sec) â”‚      6.14 â”‚      N/A â”‚       N/A â”‚       N/A â”‚       N/A â”‚       N/A â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Performance Insights

**Hardware**: G5.8xlarge (NVIDIA A10G, 24GB VRAM, 32 vCPUs, 128 GiB RAM)

**Complete Performance Scaling Analysis**:

| Concurrency | Req/sec | Tokens/sec | Avg Latency | P99 Latency | Optimal Use Case |
|-------------|---------|------------|-------------|-------------|------------------|
| 10          | 1.91    | 236.62     | 5.04s       | 9.90s       | Development/Testing |
| 20          | 4.42    | 554.70     | 4.31s       | 5.35s       | **Production (Low Latency)** |
| 40          | 6.15    | 766.65     | 6.09s       | 9.27s       | Balanced Workloads |
| 80          | 6.58    | 828.76     | 9.58s       | 20.92s      | Avoid (Poor Efficiency) |
| 120         | 7.80    | 1,007.91   | 9.95s       | 20.00s      | **Maximum Throughput** |
| 200         | 6.14    | 771.25     | 17.85s      | 30.23s      | Over-Saturated |

**Key Findings**:
- **Optimal Latency**: Concurrency 20 (4.31s avg, 5.35s P99)
- **Peak Throughput**: Concurrency 120 (1,008 tokens/sec, 7.8 req/sec)
- **Saturation Point**: Beyond C120, performance degrades significantly
- **Production Sweet Spot**: C20-40 for most real-world applications

**Generated Artifacts**:
- Performance metrics: `profile_export_genai_perf.json`
- CSV data: `profile_export_genai_perf.csv`
- Visualization plots: Time to First Token, Request Latency, Token distributions

### Expected Performance Scaling
As concurrency increases, expect:
- **Higher throughput** (more requests/sec)
- **Increased latency** (longer response times)
- **Resource saturation** at concurrency 120-200
- **Potential timeouts** under maximum load

## Hardware Specifications

- **Instance Type**: G5.8xlarge
- **GPU**: NVIDIA A10G (24GB VRAM)
- **CPU**: 32 vCPUs
- **Memory**: 128 GiB
- **Network**: Up to 25 Gbps

## Monitoring

```bash
# Check pod status
kubectl get pods -n llm

# View logs
kubectl logs -n llm -l app=llm --tail=50

# Check GPU nodes
kubectl get nodes -l instanceType=gpu
```

## Hardware Specifications

- **Instance Type**: G5.8xlarge
- **GPU**: NVIDIA A10G (24GB VRAM)
- **CPU**: 32 vCPUs
- **Memory**: 128 GiB
- **Network**: Up to 25 Gbps
